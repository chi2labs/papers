---
title: |
  \begin{center}\rule{1\linewidth}{2pt}\end{center}
  \text{Chi Square Mobile for Emergency Onlined Courses}
  \text{Preliminary Results from a Field Test}
  \begin{center}\rule{1\linewidth}{2pt}\end{center}
subtitle: |
  A Preprint
date: "April 3 2021"
keywords:
  - Emergency onlined courses
  - Learning Analytics
  - Learning Loss
  - Covid Pandemic
bibliography: references.bib
biblio-style: unsrt
output: 
  bookdown::pdf_document2:
    toc: false
    keep_tex: false
    extra_dependencies: ["float"]
---

```{r setup, include=FALSE}
library(tidyverse)
library(kableExtra)
library(magrittr)
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning = FALSE)
knitr::opts_chunk$set(fig.pos = "H", out.extra = "")
options(knitr.kable.NA = '')

## Data 
participants <- read_rds(here::here("data","users.rds"))
quiz_answers <- read_rds(here::here("data","quiz_answers.rds"))

# filter out some participants
participants <- participants %>% 
  filter(!last_name%in% c("Magadán", "Dietrichson","Pagnone"))

quiz_answers <- quiz_answers %>% #Remove our spurious answers as well
  filter(respondent_id %in% participants$user_id)

theme_set(theme_minimal())
update_geom_defaults("bar", list(fill="red"))
update_geom_defaults("line", list(color="red"))

```


```{=tex}
\begin{table}[ht]
\centering % used for centering table
\begin{tabular}{c c c c} % centered columns (2 columns)

\textbf{Aleksander Dietrichson} &  & & \textbf{Cecilia Magadán} \\ 
Chi Square Laboriatories & & & Centro de Estudios Lingüísticos en Sociedad\\
New York, NY 10023 &  &     &Universidad San Martín\\
dietrichson@gmail.com   &  & &Buenos Aires, Argentina\\
                      &   & & ceciliamagadan@gmail.com \\

\end{tabular}
\label{table:nonlin} % is used to refer this table in the text
\end{table}
```




```{=tex}
\begin{abstract}

At the onset of the COVID pandemic, thousands of courses were emergency onlined. As a response to these new and unfamiliar conditions a mobile app was developed and deployed to assist instructors in obtaining feedback from students on the evolytion of the course. This paper presents experiences and related findings from this deployment.

\end{abstract}
```

---

# Introduction 
In Argentina the COVID-19 pandemic was declared toward the end of February of 2020. In the southern hemisphere this toward the end of summer, with most schools coming off a two month summer break. The emergency measures put in place by public health authorities meant that thousands of courses which would normally be taught in a face-to-face setting would now have to be moved online. While most educational institutions in Argentina have access to learning management systems, these systems were unfamiliar to most instructors and widely deemed unsuited for the purpose. Most instructors opted for a synchronous format using some kind of videoconferencing system such as *Meet*, *Teams* or *Big Blue Button*. Legal and administrative requirements stating that a course session needed to have a start and end time as well as dates also tilted the weight in favor of synchronous options. 

# The Chi Square Mobile App 

The *Chi Square Mobile App* was originally developed to augment the feedback loop in traditional classrooms settings. Given the emerging context we decided to make the necessary changes to repurpose the development for use in the situation we found ourselves in. The app was built with several R packages, the most important ones of which were: *shinyMobile* [@shinyMobile:2020] and *chi2Mobile* [@chi2Mobile:2020] as well as *Android Studio*[@androidstudio:2020]. The necessary updates and adjustments were made before it was released in the Google play store. The system has two main interfaces, one for students and one for instructors.

## Student App
The student app prompts the student when a request for feedback is available --typically immediately following the online live session. A similar prompt is received when the deadline for completing the feedback approached. By default the deadline was five days after the request was made, and an email reminder was also sent out simultaneously with the in-app prompts. 

The template used for this field test included star-ratings on four questions, and two open-ended question where the students could enter concepts learned and request reviews of concepts that were not clearly understood. The app also allows for access to a student facing analytics dashboard, and a calendar interface. Figure \ref{fig:studentapp} shows the main application interfaces for the student.

\begin{figure}[H]
\centering
\includegraphics[width=5cm]{images/outputfile_4.png}
\includegraphics[width=5cm]{images/outputfile_5.png}
\includegraphics[width=5cm]{images/outputfile_6.png}
\caption{Main App Interfaces for the Student}
\label{fig:studentapp}
\end{figure}

## Instructor App

The instructor app provides functionality for the instructor to request feedback (or automate process this with an scheduler) using one of the templates available, or by creating a custom questionnaire. The instructor has access to summary statistics for the feedback given, including linguistic analyses for the open-ended question types. There is also a recommendation engine which will propose topics for review and make content recommendations. The instructor app also provides analytics on the progress of students, and ties in to an early warning systems to detect students at high risk of failure. Figure \@ref(fig:instructorapp) shows some of the interfaces available for the instructor.

\begin{figure}[H]
\centering
\includegraphics[width=5cm]{images/outputfile_5006.png}
\includegraphics[width=5cm]{images/outputfile_1002.png}
\includegraphics[width=5cm]{images/outputfile_2003.png}
\caption{Main App Interfaces for the Instructor}
\label{fig:instructorapp}
\end{figure}


# Context and Participants

A cohort of `r nrow(participants)` students, enrolled in *Gramatica*, an introductory level linguistic course taught by one of the authors, were asked to download and install the application on their devices. A total of `r quiz_answers$respondent_id %>% unique %>% length` complied with these instructions and responded at least once. The course had a duration of sixteen weeks, with additional final exam options at the end of the semester. Figure \ref{fig:respondents-per-week} shows the number of students who responded to the request for feedback during the semester. We see a sharp drop-off in week six, and a relatively stable number thereafter.

```{r respondents-per-week, fig.height=2, fig.cap="Respondents per Week of the Course"}
quiz_answers$date <- as.Date(quiz_answers$time_stamp)
quiz_answers$week <- lubridate::week(quiz_answers$time_stamp)
quiz_answers$week_of_course <- quiz_answers$week - min(quiz_answers$week) +1
quiz_answers %>% 
  group_by(week_of_course) %>% 
  summarize(N=n_distinct(respondent_id)) %>% 
  ggplot(aes(x=week_of_course,y=N))+
  geom_line(color="red")+
  geom_point(color="red")+
  geom_smooth(lty=2, se=FALSE,color="gray")+
  ylab("Respondents")+
  xlab("Week of the Course")+
  scale_x_continuous(breaks = c(1,5,9,13))
```

# Reponses to Star-Ratings

The student were asked to provide star-ratings (1-5) to four questions. These were:

* *Claridad de la(s) lectura(s)* [Clarity of the readings]
* *Claridad de la presentación* [Clarity of the presentation]
* *Conocimentos previos* [Prior knowledge]
* *Calidad de la conectividad* [Connectivity]

The star-ratings in these categories are shown in Figure \@ref(fig:star-ratings-per-week).

```{r star-ratings-per-week, fig.cap="Average Star-Rating per Week"}
quiz_answers %>% 
  filter(question %in% c("Calidad de la conectividad", "Claridad de la(s) lectura(s)", "Claridad de la presentación", "Conocimentos previos" )) %>% 
  group_by(week_of_course, question) %>% 
  summarize(
    "Mean Rating" = mean(as.numeric(answer),na.rm=TRUE),
    "Median Rating" = median(as.numeric(answer),na.rm=TRUE)
            ) %>% 
  ggplot(aes(x=week_of_course, y=`Mean Rating`, color = question))+
  geom_line(key_glyph = "timeseries")+
  geom_point(key_glyph  = "timeseries")+
  guides(color=guide_legend(nrow=2))+
  theme(legend.position = "top", legend.title = element_blank())+
  xlab("Week of the Course")+
  scale_x_continuous(breaks = c(1,5,9,13))+
  scale_y_continuous(limits = c(1,5))
```

## Actual Use of Scale

While the students have five options to choose from when giving a star rating, it is not necessarily clear whether they in fact use the whole scale. Figure \@ref(fig:star-ratings-overall) shows the distribution of stars for each of the four star-rated questions used in the questionnaire.

```{r star-ratings-overall, fig.cap="Comparison of Star Ratings", fig.height=3}
quiz_answers %>% 
  filter(question %in% c("Calidad de la conectividad", "Claridad de la(s) lectura(s)", "Claridad de la presentación", "Conocimentos previos" )) %>% 
  mutate(Stars = as.numeric(answer)) %>% 
  group_by(question) %>% 
  # summarize(
  #   "Minimum" = min(as.numeric(answer),na.rm=TRUE),
  #   "Maximum" = max(as.numeric(answer),na.rm=TRUE)
  # ) %>% 
  ggplot(aes(x=Stars))+
  geom_histogram()+
  theme(plot.caption = element_text(hjust = 0))+
  scale_y_continuous(labels = NULL) +
  ylab(NULL)+
  facet_wrap(~question,ncol = 2)+
  theme(
  strip.text.x = element_text(hjust=0)
  )
```

We see that the full scale of the star-ratings is only present in the answers to the question of prior knowledge. 


# Linguistic Analysis

The feedback from students also included two open-ended questions. These were:

* *Conceptos que aprendiste* [Concepts learned]
* *Conceptos que necesitás revisar* [Concepts for review]

The interface provided to the students allows for input of any length, i.e. the individual concepts could be multiple words, (see Figure \@ref(fig:studentapp)), separated by the <enter> key. Once enter was hit, the word or words were visually indicated to be of the same group. The interaction described is *standard* for this type of input on a mobile device.

The purpose of this question and the selected interface was to elicit key-terms for each category as this would facilitate automatization of the linguistic analysis. In practice, however, some proportion of the did not enter the data as expected. It is not clear whether this was because the field format was deemed inappropriate for the purpose, the instructions were unclear or due to lack of familiarity with this type of interface. A software update was made to the instructor app (Figure \@ref(fig:instructorapp)) to enhance the interpretability of the feedback, however, the researchers chose not to make any changes to the student interface.

Several different *feedback formats* were observed. While most students used the interface as intended (*Norm*), some chose not to answer the open-ended questions, some chose to submit a list, separated by commas and/or other connectors, and some chose a longer-form style of feedback. Table \@ref(tab:input-strategy-used) summarizes these.

```{r deduplication}
# Deduplication 
# Turns out there was a bug in the code that led to duplicated answers for the linguistic input. This was fixed after week 2
quiz_answers %>% 
  filter(question_type == "KW") %>% 
  select(question, answer, week_of_course, request_id, 
         respondent_id, question_id) %>% 
 group_by(respondent_id, request_id, question_id, question, answer,week_of_course) %>% 
  tally() %>% 
  arrange(desc(desc(week_of_course),desc(n)))  -> quiz_answers2

```

```{r}
quiz_answers2 <- quiz_answers2 %>%
  select(answer,everything())

quiz_answers2 <- quiz_answers2 %>% 
  mutate(answer2 = ifelse(answer =="NULL", NA,answer) ) %>%  # No answer
  mutate(response_type = 
           case_when(
             is.na(answer2) ~ "N/A",
             startsWith(answer2,"c(") ~ "Norm",
             stringr::str_detect(answer2, ",") > 0 ~ "Comma",
             stringr::str_detect(answer2, " e ") > 0 ~ "Comma", #connector
             stringr::str_detect(answer2, " y ") > 0 ~ "Comma", #connector
             stringr::str_detect(answer2, " vs. ") > 0 ~ "Comma", #connector
             stringr::str_detect(answer2, " - ") > 0 ~ "Comma", #connector
             stringr::str_detect(answer2," ", negate = TRUE) ~ "Norm"
             #, #Single word
             ,stringr::str_count(answer2," ") == 1 ~ "Norm" #two words
             ,stringr::str_count(answer2," ") == 2 ~ "Norm" #three words
             ,TRUE ~ "Long form"
           )
  ) 

# I tried doing this with the tidyverse case_when + lengt(eval(...etc. But it seems evaluation always happens first so
tmp <- quiz_answers2 %>% 
  select(respondent_id, request_id, question_id,answer2) %>% 
  filter(startsWith(answer2,"c(")) %>% 
  mutate(concept_count = length(eval(parse(text=answer2)))) %>% 
  mutate(concepts =
           stringr::str_flatten(eval(parse(text=answer2)),collapse="|" )
  )

quiz_answers2 <- quiz_answers2 %>% 
  left_join(tmp)

# Now parse the rest
tmp2 <- quiz_answers2 %>% 
  mutate(
    concept_count = 
      case_when(
        !is.na(concept_count) ~ concept_count, #i.e. do nothing
        response_type == "Comma" ~ length(stringr::str_split(answer2,"\\sy\\s|\\so\\s|,|vs\\.|\\/")),
        response_type == "Norm" & is.na(concept_count) ~ 
          length(stringr::str_split(answer2," "))
        #  ,TRUE ~ length(stringr::str_split(answer2," "))
      )
  ) %>% 
  mutate(
    concepts = 
      case_when(
        !is.na(concept_count) ~ concepts, #i.e. do nothing
        response_type == "Comma" ~ stringr::str_replace_all(answer2,"\\sy\\s|\\so\\s|,|vs\\.", " "),
        response_type == "Norm" & is.na(concept_count) ~ answer2
        #  ,TRUE ~ length(stringr::str_split(answer2," "))
      )
  )  

quiz_answers2 <- tmp2
rm(tmp);rm(tmp2);
```

```{r input-strategy-used, }
  quiz_answers2 %>% 
    group_by(response_type, question) %>% 
    count %>% 
    rename(Question=question) %>% 
    pivot_wider(names_from = "response_type", values_from = "n") %>% 
  select(Question,`N/A`, Norm, Comma, `Long form`) %>%
    knitr::kable(caption = "Input Strategies Observed",
                 booktabs = TRUE) %>%
    add_header_above(c(" ", " ", "Format" = 3), bold = TRUE, italic = TRUE) %>% 
    kable_styling(latex_options = "hold_position")
```
Except for the non-reply, which is a legitimate answer to any of the questions posed, the format of the feedback is significant to the type of processing that is needed to extract, summarize and analyze the linguistic data.
```{r}
quiz_answers2 %>% 
  filter(!is.na(concept_count)) %>% 
  group_by(week_of_course,question) %>% 
  summarize(
    concepts = sum(concept_count,na.rm = TRUE),
    respondents = n_distinct(respondent_id)
  ) %>% 
  mutate(concepts_per_student = concepts/respondents) -> tmp

mean_learned <- tmp %>% filter(question == "Conceptos que aprendiste") %>% pull(concepts_per_student) %>% mean %>% 
  round(digits=1)
concepts_requested <- tmp %>% filter(question != "Conceptos que aprendiste") %>% pull(concepts_per_student) 

```


## Number of Concepts Learned and Requested

We parsed the data using the appropriate technique depending on the input strategy used by the students. We then counted the number of concepts in all non-empty inputs across the fourteen weeks of the course. Figure \@ref(fig:concepts-per-student) summarizes these results. We see that requests for review of concepts remained relatively stable throughout the course, with a mean of `r mean(concepts_requested) %>% round(digits=1)` and a standard deviation of `r sd(concepts_requested) %>% round(digits=1)`, while the number of concepts learned show a significant increase in weeks 9-11. It is also worth noting that at no point during the course do the students on average report learning fewer concepts than those needing review.


```{r concepts-per-student, fig.cap="Concepts Input per Student Per Week of the Course"}
tmp %>% 
  ggplot(aes(x = week_of_course, y = concepts_per_student, color=question ))+
  geom_line(key_glyph = "timeseries")+
  geom_point(key_glyph  = "timeseries")+
  theme(legend.position = "top", legend.title = element_blank())+
  xlab("Week of the Course")+
  ylab("Number of Concepts")+
  scale_x_continuous(breaks = c(1,5,9,13))+
  scale_y_continuous(limits = c(0,15))
```

## N-gram Analysis

```{r}
library(tidytext)
## Stop_words for unigrams
stop_words_unigrams <- data.frame(
  ngram = c(
    "que",
    "y",
    "de",
    "la",
    "los",
    "las",
    "al",
    "del",
    "un",
    "una",
    "a",
    "vs",
    "me",
    "sobre",
    "es",
    "no",
    "o",
    "e",
    "en",
    "se",
    "entre",
    "como",
    "con",
    "el",
    "d",
    "ella")
)

# TODO: Something with accents - they are not consistently added.

unigrams <-
  quiz_answers2 %>%
  ungroup %>% 
  select(concepts, question, week_of_course) %>% 
  unnest_tokens(ngram, concepts, token = "words" ) %>% 
  anti_join(stop_words_unigrams) 

```


```{r}
bigrams <-
  quiz_answers2 %>%
  ungroup %>% 
  unnest_tokens(ngram, concepts, token = "ngrams", ngram_delim=" ", n = 2L) %>% 
  select(ngram,question, week_of_course)
```

```{r}
trigrams <-
  quiz_answers2 %>%
  ungroup %>% 
  unnest_tokens(ngram, concepts, token = "ngrams", ngram_delim=" ", n = 3L) %>% 
  select(ngram,question, week_of_course)

```


```{r}
.extract_top_ngrams_by_week_of_course <- function(ngrams, max_n=10) {
  ngrams %>%
    group_by(week_of_course,ngram) %>%
    tally %>%
    filter(!is.na(ngram)) %>%
    group_by(week_of_course) %>%
    arrange(desc(n),.by_group = TRUE) %>%
    filter(row_number()<= max_n) %>% 
    arrange(ngram, .by_group = TRUE)
}

  
  .create_ngram_matrix_old <- function(tmp, ncol = 7) {
    # 7 column matrix
    ret  <- matrix(nrow = 22, ncol = 7)
    # Insert "headings"
    ret[1,] <- paste0("Week ",1:7)
    ret[2:11,] <- tmp$ngram[1:70]
    ret[12,] <- paste0("Week ",8:14)
    ret[13:22,] <- tmp$ngram[71:140]
    ret 
  }
```

```{r}

.create_ngram_matrix <- function(x, n_col = 7, header_prefix = "Week ") {
  # Create layout we want c(1,8,2,9,3,10 etc
  max_weeks <- max(x$week_of_course)
  n_headers <- ceiling(max_weeks/n_col)
  rows_per_week <- x %>% group_by(week_of_course) %>% count %>% pull(n) %>% max
  my_seq <-   matrix(ncol = n_col, data=1:max_weeks, byrow = TRUE) %>% as.vector()
  my_matrix <- matrix(nrow=(rows_per_week*n_headers+n_headers), ncol=n_col)
  #insert each week in place
 
  for(i in 1:length(my_seq)){
    my_week <- my_seq[i]
    my_data <- x %>%
      filter(week_of_course == my_week) %>% pull(ngram)
    my_data <- c(paste0(header_prefix,my_week),my_data)
    # Make sure it is right length 
    length(my_data) <- rows_per_week+1 # fills up with na
    
    row_num_max <- ceiling(my_week/n_col)*rows_per_week+
      ceiling(my_week/n_col) #header rows need space as well
    my_rows <- (row_num_max-rows_per_week):row_num_max
    my_col <- my_week%%n_col
    if(my_col == 0 )my_col = 7
    my_matrix[my_rows,my_col] <- my_data
 
  }
  my_matrix
} 
```


N-gram analysis was performed [@tidytext:2016] on the feedback data. The results from the unigram analysis of *concepts learned* are summarized in Table \@ref(tab:unigram-table-concepts-learned), and those for *review requests* are summarized in Table \@ref(tab:unigram-table-review-requests). 
```{r unigram-table-concepts-learned}
  unigrams %>% 
    filter(question == "Conceptos que aprendiste" ) %>% 
    .extract_top_ngrams_by_week_of_course() %>% 
    .create_ngram_matrix() %>% 
    knitr::kable(booktabs=TRUE, align = 'l', 
                 caption = "Reported Concepts Learned per Week",
                 linesep = c(rep("",10),"\\addlinespace")) %>% 
    row_spec(c(1,12), bold=TRUE) %>% 
    kable_styling(latex_options = c( "scale_down", "hold_position"))
  #kable_styling(latex_options = "hold_position")


```
```{r unigram-table-review-requests}

  unigrams %>% 
    filter(question != "Conceptos que aprendiste" ) %>% 
    .extract_top_ngrams_by_week_of_course() %>% 
    .create_ngram_matrix() %>% 
    knitr::kable(booktabs=TRUE, align = 'l', 
                 caption = "Requests for Review",
                 linesep = c(rep("",10),"\\addlinespace")) %>% 
    row_spec(c(1,12), bold=TRUE) %>% 
    kable_styling(latex_options = c( "scale_down", "hold_position")) 
#    kable_styling(latex_options = "hold_position")

```
We have included these summaries for illustrative purposes, while recognizing that they may be difficult to interpret without knowledge of the course curriculum in question. Bigram and trigram analyses were also performed, but have not reported here.

It was generally found that the concepts and topics that students reported having learned were in line with the instructor's expectations and that the review requests were particularly useful to the instructor in organizing course. The lack of any (relevant) review requests for weeks seven, nine and ten are also related to the organization of the course, as evaluations were due around this time. Finally we note that in week thirteen several of the students chose to use the *request for feedback* field in the app to write thank you notes to the instructor and teaching assistants. This is yet another example of how the user is king when it comes to interacting with computer systems: they will use however they please, and ways that make sense to them, irregardless of the intent of the designer of the system.


```{r, include = FALSE}
outcome_data <- read_rds(here::here("data","grades_gramatica_2011_2020.rds"))
grades_2020 <- outcome_data %>% 
  filter(year == 2020)
grades_2020 <- grades_2020 %>% 
  mutate(passfail = ifelse(promedio_cursada_numeric>=4,1,0)) %>% 
  mutate(passfail = ifelse(examen_final_numeric>=4 & !is.na(examen_final_numeric), 1, passfail)) %>% 
  mutate(passfail = ifelse(is.na(passfail),0,passfail))


#join the data
my_data <- grades_2020 %>% 
  left_join(participants, by = c(e_mail = "email"))
# Let's start with a logistic regression model
quiz_answers2 %>% 
  group_by(question, respondent_id) %>% 
  summarize(
    concept_count = sum(concept_count,na.rm=TRUE)
  ) %>% 
  pivot_wider(names_from = question, values_from = concept_count) %>% 
  rename(learning_reported = 2, review_requests = 3) %>% 
  right_join(my_data, by = c(respondent_id = "user_id"))  %>% 
  select(respondent_id,learning_reported, review_requests,passfail,promedio_cursada_numeric, examen_final_numeric) %>% 
  replace_na(list(learning_reported = 0,review_requests = 0)) %>% 
  mutate(
    promedio_cursada_numeric = if_else(promedio_cursada_numeric>=4,promedio_cursada_numeric,0) ) %>% 
  
  mutate(course_grade = promedio_cursada_numeric) %>%
  rowwise() %>% 
  mutate(course_grade = max(examen_final_numeric,promedio_cursada_numeric, na.rm = TRUE)) %>% 
  mutate(course_grade = ifelse(course_grade>=4,course_grade,0)) -> my_data
```

# Statistical Modeling

```{r shapiro}
my_data$course_grade %>% 
  shapiro.test() ->shapiro_test
```
As pointed out by e.g. @dietrichson_2020blog distribution of grades do typically not conform to statistical normality. In the case of the data at our disposal, the grades are ostensibly given on a scale from zero to ten, with four being the lowest passing grade. In practice the lower end of the scale is not used and a two (2) is often granted as a universal *Fail*, and zero (0) suggests withdrawal from the course on the part of the student. The Shapiro Wilk normality test was performed on the course grades and confirmed that course grades were indeed not normally distributed (*W* = `r shapiro_test$statistic %>% signif(4)%>% stringr::str_remove("0")`, *p* = `r shapiro_test$p.value %>% signif(3) %>% format(scientific = FALSE) %>% stringr::str_remove("0")`).

For our statistical modeling we therefore assumed that the variable *course grade* would be measured on an ordinal scale, with the range (0,4] encoded as *Fail*. 

## Kendall's Tau

We computed the raw number of *concepts learned* and *review requests*. To explore the relationship between this measure and course grade we calculated the Kendall rank correlation coefficient (Kendall's $\tau$) between these variables and *course grade*. The results are shown in Table \@ref(tab:kendall-tau). 


```{r kendall-tau}

my_data %$%
  cor.test(course_grade,learning_reported,method="ken") %>%
  broom::tidy() %>%
  mutate(Variable = "Learning Reported") %>%
  select(Variable ,Tau=estimate, "z-score"=statistic, "p-value"=p.value) %>%
  union(
    my_data %$%
      cor.test(course_grade,review_requests,method="ken") %>%
      broom::tidy() %>%
      mutate(Variable = "Review Requests") %>%
      select(Variable ,Tau=estimate, "z-score"=statistic, "p-value"=p.value)
  ) %>%
  knitr::kable(caption="Kendalls $\\tau$", booktabs=TRUE, digits = 4) %>%
  kable_styling() %>%
  add_header_above(c(" "=1,"Test statistics"=3)) %>% 
    kable_styling(latex_options = "hold_position")
``` 

While the correlation coefficients cannot be interpreted as a percentage (as in the case with Pearson product-moment correlations), $\tau$ values above .3 are generally considered a *strong correlation* [@Puka2011]. The z-score and corresponding p values show that there is a statistically significant relationship between the two variables in question, i.e. each of *learning reported* and *review requests* on the one hand and *course grade* on the other.



## Ordinal Regression

While there is an overall strong and significant correlation between the variables in question and the outcome variable, we wanted explore these relationships in more detail. We therefore fitted an ordinal regression model [@MASS2021; @williams:2020] with the raw number of concepts learned and review requests as independent variables. This approach allowed us to estimate an effect size for the coefficient as well as the different intercepts. We are also able to deal with potential interaction effects between the independent variables. The results of the regression analysis are reported in Table \@ref(tab:ordinal-regression), with effect sizes reported as log-odds.

```{r, include = FALSE}
model_2data <- my_data
# Find the levels in outcome variable
model_2data$course_grade %>% unique %>% sort -> my_levels

# model_2data <- model_2data %>%
#   filter(!is.na(respondent_id)) 
model_2data$course_grade <- factor(
  model_2data$course_grade
  ,
  levels = my_levels,
  labels = c("Fail", my_levels[2:length(my_levels)])
  ,ordered = TRUE
)

library(MASS)
 model_2 <-  polr(course_grade ~ (review_requests + learning_reported)^2,
                  data=model_2data)
  summary(model_2)
detach("package:MASS", unload = TRUE)
### P-values
#ctable <- coef(summary(model_2))
#p <- dt(abs(ctable[, "t value"]),df=nrow(model_2data)-1)

```

```{r ordinal-regression}
broom::tidy(model_2) %>% 
  rename(Term=term, Estimate = estimate, "Std Error"=std.error, "T-value" = statistic ) %>% 
  mutate(Term = replace(Term,Term=="review_requests","Review Requests")) %>% 
  mutate(Term = replace(Term,Term=="learning_reported","Learning Reported")) %>% 
  mutate(Term = replace(Term,Term=="review_requests:learning_reported","Interaction")) %>% 
  select(-coef.type) %>% 
  knitr::kable(caption = "Summary for Ordinal Regression Model", 
               booktabs = TRUE,
               digits=3) %>% 
    kable_styling(latex_options = "hold_position") %>% 
  pack_rows("Coefficients",1,3) %>% 
  pack_rows("Intercepts",4,10)
```

We see that there is a negligible interaction effect, furthermore it is worth noting that the effect size for *review requests* is significantly higher than that for *concepts learned* (Figure \@ref(fig:concept-count-vs-final-grade). As for the intercept estimates, we see that they increase in concert with the intercept thresholds. This might indicate that a working feedback loop, like the one our system aspires to provide, benefit the stronger students more than the weaker ones. 

```{r, include = FALSE}
cor.test(model_2data$review_requests, 
         as.numeric(model_2data$course_grade), method = "kendall") %>% 
  broom::tidy()

cor.test(model_2data$learning_reported,as.numeric(model_2data$course_grade), method = "kendall")
```


```{r concept-count-vs-final-grade, fig.cap="Concepts Learned/Review Requests by Course Grade"}
my_data %>% 
  pivot_longer(c(review_requests, learning_reported)) %>% 
  ggplot(aes(y=course_grade, x=value, color=name))+
  geom_point(alpha=.3,key_glyph="path")+
  geom_smooth(method = "lm",lty=2, se=FALSE,
              key_glyph=NULL)+
  theme(legend.position = "top", legend.title = element_blank())+
  scale_y_continuous(breaks = 0:10, labels = 0:10,limits = c(0,10))+
  ylab("Course Grade")+
  xlab("Concept Count")+
  scale_color_discrete(labels = c(learning_reported = "Learning Reported",review_requests = "Review Requests"))
```

## Limitations

It is important to keep in mind that while the results reported are congruent with our hypothesis that a working feedback loop is beneficial to both students and instructors. However, the results reported are *correlational* we cannot infer *causation* given the observational nature of this study. 

# Conclusion

The results reported in this study are certainly encouraging. We see correlational evidence congruent with our hypothesis that the mobile app can indeed create a workable feedback loop, and that this may be beneficial to the student. Further research is clearly merited to draw inferences about the usefulness of the software for the *instructor*, a question that we were unable to explore empirically given our N of 1 instructor.


# References
